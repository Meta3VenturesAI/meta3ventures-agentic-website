#!/usr/bin/env node

/**
 * Debug Agent LLM Flow - Trace exactly what happens when agents try to use LLMs
 */

console.log('ğŸ” DEBUGGING AGENT LLM FLOW');
console.log('=' .repeat(60));

// Test 1: Check if LLM Service can be loaded
console.log('\n1. Testing LLM Service Import...');

try {
    // This simulates what happens in the browser
    console.log('âœ… Would import LLMService from browser context');
    console.log('ğŸ“ File: src/services/agents/refactored/LLMService.ts');

    // Test 2: Check fallback chain priority
    console.log('\n2. Testing Fallback Chain...');
    const fallbackChain = ['ollama', 'deepseek', 'groq', 'huggingface', 'localai', 'openai', 'anthropic', 'cohere', 'replicate'];
    console.log('ğŸ“‹ Provider Priority:', fallbackChain);

    // Test 3: Check what happens with Ollama provider
    console.log('\n3. Testing Ollama Provider...');
    console.log('ğŸ”— Expected baseUrl:', 'http://localhost:11434');
    console.log('ğŸ¤– Expected model:', 'qwen2.5:latest');

    // Test 4: Simulate the actual agent flow
    console.log('\n4. Simulating Agent LLM Request Flow...');
    console.log('ğŸ“¥ Agent calls: llmService.generateResponse()');
    console.log('ğŸ” LLMService looks for provider supporting: qwen2.5:latest');
    console.log('ğŸ¯ Should find: Ollama provider');
    console.log('ğŸ“¡ Should call: OllamaProvider.generateResponse()');
    console.log('ğŸŒ Should POST to: http://localhost:11434/api/generate');

    // Test 5: Check what would happen if Ollama fails
    console.log('\n5. Fallback Behavior...');
    console.log('âŒ If Ollama fails â†’ Try DeepSeek (needs API key)');
    console.log('âŒ If DeepSeek fails â†’ Try Groq (needs API key)');
    console.log('âŒ If all fail â†’ Use FallbackAgentService');

    console.log('\n6. Expected vs Actual Behavior...');
    console.log('ğŸ¯ EXPECTED: Agent gets LLM response from Qwen2.5');
    console.log('âš ï¸ ACTUAL: Agent falls back to pre-configured responses');
    console.log('ğŸ› LIKELY ISSUE: LLM providers failing, triggering fallback mode');

    console.log('\n7. Diagnostic Questions...');
    console.log('â“ Is llmService.generateResponse() actually being called?');
    console.log('â“ Are all LLM providers failing health checks?');
    console.log('â“ Is the agent using fallback responses instead of LLM?');
    console.log('â“ Are there errors in the browser console?');

} catch (error) {
    console.log('âŒ Failed to analyze LLM flow:', error.message);
}

console.log('\n' + '=' .repeat(60));
console.log('ğŸ§ª NEXT STEPS TO DEBUG:');
console.log('1. Check browser console at: http://localhost:3000/manual-agent-test.html');
console.log('2. Look for LLM-related errors when testing agents');
console.log('3. Verify if generateLLMResponse() is being called');
console.log('4. Check if agents are hitting try/catch fallback blocks');
console.log('5. Confirm agent proxy function is working correctly');
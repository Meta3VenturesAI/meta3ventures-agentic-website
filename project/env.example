# Meta3 Ventures - Environment Configuration
# Copy this file to .env and configure your settings

# =================================
# OPEN SOURCE LLM PROVIDERS
# =================================

# Ollama (Local LLM Server)
# Download from: https://ollama.ai/
# Default models: llama3.2:3b, llama3.1:8b, mistral:7b, codellama:7b
OLLAMA_URL=http://localhost:11434

# LocalAI (Self-hosted OpenAI Compatible API)  
# GitHub: https://github.com/go-skynet/LocalAI
# Supports: ggml models, Alpaca, Vicuna, GPT4All
LOCALAI_URL=http://localhost:8080

# VLLM (High-performance LLM Serving)
# GitHub: https://github.com/vllm-project/vllm
# Supports: Llama-2, Mistral, CodeLlama, and more
VLLM_URL=http://localhost:8000

# HuggingFace Inference API (Free Tier Available)
# Get token from: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=your_huggingface_token_here

# =================================
# AGENT SYSTEM CONFIGURATION
# =================================

# Enable/disable the entire agent system
VITE_AGENTS_DISABLED=false

# Enable fallback responses when LLM providers are unavailable
VITE_ENABLE_FALLBACK=true

# Agent proxy endpoint (relative path for Netlify functions)
VITE_AGENT_PROXY_PATH=/.netlify/functions/agent-proxy

# Default model preferences
VITE_DEFAULT_OLLAMA_MODEL=llama3.2:3b
VITE_DEFAULT_LOCALAI_MODEL=ggml-gpt4all-j
VITE_DEFAULT_VLLM_MODEL=meta-llama/Llama-2-7b-chat-hf
